model:
  type: transformer
  vocab_size: 1000
  hidden_size: 256
  num_layers: 6
  num_heads: 8
  max_length: 512
  ffn_size: 1024
  attention_dropout: 0.1
  ffn_dropout: 0.1
  eos_token_id: 2
  bos_token_id: 3
  pad_token_id: 1
  unk_token_id: 0
training:
  num_epochs: 10
  batch_size: 32
  learning_rate: 1.58e-07
  weight_decay: 0.01
  optimizer: adamw
  betas:
  - 0.9
  - 0.999
  scheduler: warmup_cosine
  warmup_steps: 1000
  total_steps: 10000
  min_lr: 0.0
  gradient_clip: 1.0
  seed: 42
  device: cuda
  use_amp: true
  log_interval: 100
  save_model: true
  save_path: models/best_model.pt
  save_best: true
  best_model_path: models/best_model.pt
  save_checkpoints: true
  checkpoint_interval: 5
data:
  type: text
  train_path: null
  val_path: null
  max_length: 128
  stride: 64
  vocab_size: 1000
  image_size: 224
  num_classes: 10
  label_file: null
  num_workers: 0
  pin_memory: true
  num_examples: 1000
logging:
  level: INFO
  wandb: false
  project_name: tiny-ai-trainer
  run_name: null
  tags: []
  log_interval: 100
