# Tiny AI Model Trainer

A powerful CLI tool for training small Language Models (LLMs) and Vision models using custom YAML configurations for reproducible research.

## Features

- ğŸš€ **Multi-Model Support**: Train both LLMs and Vision models
- âš™ï¸ **Hydra Configuration**: YAML-based configs for reproducible experiments
- ğŸ“Š **Experiment Tracking**: Integrated wandb logging and TensorBoard support
- ğŸ”§ **Modular Architecture**: Easy to extend and customize
- ğŸ“ˆ **Rich Logging**: Beautiful CLI output with progress tracking
- ğŸ¯ **Reproducible Research**: Deterministic training with seed management

## Quick Start

### Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd TinyAI

# Install dependencies
pip install -r requirements.txt

# Install the package in development mode
pip install -e .
```

### Basic Usage

```bash
# Train an LLM with default config
python -m tinyai.train model=llm

# Train a vision model with custom config
python -m tinyai.train model=vision config=vision_custom

# Override config parameters
python -m tinyai.train model=llm training.batch_size=32 training.learning_rate=1e-4

# Run with wandb logging
python -m tinyai.train model=llm logging.wandb=true logging.project_name=my_experiment
```

## Configuration

The trainer uses Hydra for configuration management. Configs are stored in `configs/` directory:

```
configs/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ llm.yaml
â”‚   â””â”€â”€ vision.yaml
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ default.yaml
â”‚   â””â”€â”€ custom.yaml
â””â”€â”€ logging/
    â””â”€â”€ default.yaml
```

### Example Config Structure

```yaml
# configs/model/llm.yaml
defaults:
  - training: default
  - logging: default

model:
  type: "transformer"
  vocab_size: 50257
  hidden_size: 768
  num_layers: 12
  num_heads: 12

training:
  batch_size: 16
  learning_rate: 1e-4
  num_epochs: 10
  warmup_steps: 1000

data:
  train_path: "data/train.txt"
  val_path: "data/val.txt"
  max_length: 512

logging:
  wandb: true
  project_name: "tiny-llm"
  log_interval: 100
```

## Project Structure

```
TinyAI/
â”œâ”€â”€ tinyai/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py              # Main training script
â”‚   â”œâ”€â”€ models/               # Model implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm.py
â”‚   â”‚   â””â”€â”€ vision.py
â”‚   â”œâ”€â”€ data/                 # Data loading utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ datasets.py
â”‚   â”‚   â””â”€â”€ tokenizers.py
â”‚   â”œâ”€â”€ training/             # Training utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â””â”€â”€ optimizer.py
â”‚   â””â”€â”€ utils/                # Utility functions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logging.py
â”‚       â””â”€â”€ metrics.py
â”œâ”€â”€ configs/                  # Hydra configurations
â”œâ”€â”€ scripts/                  # Utility scripts
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## Advanced Usage

### Custom Model Implementation

```python
# tinyai/models/custom_model.py
from tinyai.models.base import BaseModel

class CustomModel(BaseModel):
    def __init__(self, config):
        super().__init__(config)
        # Your model implementation
        
    def forward(self, x):
        # Forward pass
        return output
```

### Custom Training Loop

```python
# Override training behavior
class CustomTrainer(Trainer):
    def training_step(self, batch):
        # Custom training logic
        pass
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## License

MIT License - see LICENSE file for details. 